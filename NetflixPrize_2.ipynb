{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from scipy.sparse import coo_matrix, hstack,vstack\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Veronika_Revyakina\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           User_Id  Rating  Movie_Id\n",
      "1          1488844     3.0         1\n",
      "5000996     501954     2.0       996\n",
      "10001962    404654     5.0      1962\n",
      "15002876    886608     2.0      2876\n",
      "20003825   1193835     2.0      3825\n",
      "25004661   1899206     3.0      4661\n",
      "30005496    154804     4.0      5496\n",
      "35006274   2078749     5.0      6274\n",
      "40007057    450763     5.0      7057\n",
      "45007991    102092     3.0      7991\n",
      "50009023    220298     5.0      9023\n",
      "55010042    550530     5.0     10042\n",
      "60011038    222570     3.0     11038\n",
      "65011875   1273080     5.0     11875\n",
      "70012676   2026970     5.0     12676\n",
      "75013582    506044     4.0     13582\n",
      "80014453    353605     2.0     14453\n",
      "85015116    664606     3.0     15116\n",
      "90016008   2213715     3.0     16008\n",
      "95016879   1589401     5.0     16879\n",
      "100017627  2314006     4.0     17627\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"NetflixProcessedDataFile.csv\",index_col=0).drop_duplicates()\n",
    "print(df.iloc[::5000000, :])#START:STOP:STEP (5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of column User_id\n",
    "#one_hot_users = pd.get_dummies(df1['User_Id'])\n",
    "#one_hot_users\n",
    "# Drop column User_Id as it is now encoded\n",
    "#df1 = df1.drop('User_Id',axis = 1)\n",
    "# Join the encoded df\n",
    "#df1 = df1.join(one_hot_users)\n",
    "#df1  \n",
    "\n",
    "#This approach with dummies wouldnt work, because dataset is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100480507, 17770)\n"
     ]
    }
   ],
   "source": [
    "# importing one hot encoder from sklearn \n",
    "encoder = OneHotEncoder(categories='auto') \n",
    "#X-train matrix for encoder to learn categories\n",
    "#enc.fit(X)  #process of encoder learning categories\n",
    "one_hot_user_matrix = encoder.fit_transform(np.asarray(df['User_Id']).reshape(-1,1)) \n",
    "one_hot_movie_matrix = encoder.fit_transform(np.asarray(df['Movie_Id']).reshape(-1,1)) \n",
    "print(one_hot_movie_matrix.shape) #amount of rows=amount of ratings , amount of columns=amount of unique user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100480507, 497959)\n",
      "(100480507, 1)\n"
     ]
    }
   ],
   "source": [
    "X=hstack([one_hot_user_matrix,one_hot_movie_matrix]) #24053764x475257 matrix joined user_id and movie_id 2 groups\n",
    "#475257=amount of users+amount of films\n",
    "#24053764=amount of non null ratings\n",
    "#create ratings vector \n",
    "ratings=np.asarray(df['Rating']).reshape(-1,1)\n",
    "#ratings=df['Rating']\n",
    "print(X.shape)\n",
    "print(ratings.shape)\n",
    "X,ratings=shuffle(X,ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_predict(X,omega,omega0,V,sum_const):\n",
    "    prod=X.dot(omega)#(24053764,1)\n",
    "    linear_part=omega0+prod\n",
    "    #V_part=np.square(sum_const)-X.power(2).dot(np.square(V)) # (24053764,3)\n",
    "    V_part=(sum_const**2)-X.power(2).dot((V)**2) \n",
    "    sqr_part=(0.5*np.sum(V_part,axis=1)).reshape(-1,1) # (24053764,1)\n",
    "    return linear_part+sqr_part # (24053764,1)\n",
    "    \n",
    "def root_mean_square_error(ratings_predict,ratings_actual):\n",
    "    #ratings_predict=svd_predict(X,omega,omega0,V)\n",
    "    #summands=np.power(ratings_actual-ratings_predict,2)\n",
    "    #return np.sqrt(mean_squared_error(ratings_actual, ratings_predict))\n",
    "    mse=np.sum(np.power(ratings_actual-ratings_predict,2))/len(ratings_predict)\n",
    "    return np.sqrt(mse)\n",
    "   \n",
    "def r_squared(ratings_predict,ratings_actual):\n",
    "    #ratings_predict=svd_predict(X,omega,omega0,V)\n",
    "    nom=np.sum(np.power(ratings_actual-ratings_predict,2))\n",
    "    denom=np.sum(np.power(ratings_actual-np.mean(ratings_actual,axis=0),2))\n",
    "    return 1-nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train,Y_train,K,learning_rate,num_epoches,batch_size):\n",
    "    #omega=np.random.sample((X_train.shape[1],1)) \n",
    "    #omega=(np.array([0.5]*(X_train.shape[1])).reshape(X_train.shape[1],1))\n",
    "    omega=np.full((X_train.shape[1], 1), 0.5)\n",
    "    omega0=0.5\n",
    "    #V=np.random.sample((X_train.shape[1],K))\n",
    "    #V=(np.array([0.5]*(X_train.shape[1]*K)).reshape(X_train.shape[1],K))\n",
    "    V=np.full((X_train.shape[1], K), 0.5)\n",
    "    r2_tr=[]\n",
    "    rmse_tr=[]\n",
    "   \n",
    "    it=0\n",
    "    for it in range(num_epoches): \n",
    "        #data = sparse.hstack((X_train, Y_train)).tocsr()\n",
    "        #np.random.shuffle(data) #shuffle at each epoch (not every batch creation)\n",
    "        #shuffle_data(data)\n",
    "        X_train,Y_train=shuffle(X_train,Y_train)\n",
    "        n_minibatches = X_train.shape[0] // batch_size \n",
    "        i = 0\n",
    "        for i in range(n_minibatches + 1):\n",
    "            X_mini=X_train.tocsr()[i * batch_size:(i + 1)*batch_size, :] \n",
    "            y_mini=Y_train[i * batch_size:(i + 1)*batch_size,:]\n",
    "            #print(np.shape(X_mini))   \n",
    "            #print(np.shape(y_mini)) \n",
    "            N=X_mini.shape[0] # 24053764\n",
    "            #PARAMETERS\n",
    "            sum_const=X_mini.dot(V)\n",
    "            ratings_predict=svd_predict(X_mini,omega,omega0,V,sum_const)\n",
    "            error=y_mini-ratings_predict #(24053764,1)\n",
    "            omega=omega+learning_rate*2*((X_mini.T).dot(error))/N #(475257,1)\n",
    "            omega0=omega0+learning_rate*2*np.sum(error)/N #(1,1)\n",
    "            V = V + learning_rate*2*((X_mini.T.dot(np.multiply(error,sum_const)))-np.multiply(V, X_mini.T.dot(error)))/N\n",
    "\n",
    "            ratings_predict=svd_predict(X_mini,omega,omega0,V,sum_const)\n",
    "            r2_tr.append(r_squared(ratings_predict, y_mini))\n",
    "            rmse_tr.append(root_mean_square_error(ratings_predict, y_mini))\n",
    "            #print(rmse_tr)\n",
    "            #print(r2_tr)\n",
    "    return omega,omega0,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469.857412815094\n",
      "0.010662515055604116 1.0793783636805452 0.01067270100561446 1.0794216932390956\n",
      "474.4721620082855\n",
      "0.010659036708014491 1.0793890778481028 0.010674899707278529 1.0794182768248748\n",
      "474.1438162326813\n",
      "0.010674330676887833 1.0794801291164673 0.010664257958878487 1.0793992479912322\n",
      "471.3788721561432\n",
      "0.010671055953146436 1.0793984197454107 0.010667797170874072 1.0794181907338827\n",
      "503.83174419403076\n",
      "0.010664635786712728 1.0794309138930966 0.010672378610222744 1.079408431481405\n"
     ]
    }
   ],
   "source": [
    "r2=[]\n",
    "rmse=[]\n",
    "omega=[]\n",
    "omega0=[]\n",
    "r2_train=[]\n",
    "rmse_train=[]\n",
    "cross_val_parts_amount=5\n",
    "cross_val_part_size=X.shape[0]//cross_val_parts_amount\n",
    "item=0\n",
    "for item in range(cross_val_parts_amount):\n",
    "    X_test=X.tocsr()[item*cross_val_part_size:(item+1)*cross_val_part_size,:]\n",
    "    Y_test=ratings[item*cross_val_part_size:(item+1)*cross_val_part_size]\n",
    "    X_train=vstack([X.tocsr()[0:(item)*cross_val_part_size,:], X.tocsr()[(item+1)*cross_val_part_size:X.shape[0],:]], 'csr') \n",
    "    #Y_train=np.delete(ratings,ratings[item*cross_val_part_size:(item+1)*cross_val_part_size])\n",
    "    #Y_train=ratings.drop(ratings.index[item*cross_val_part_size:(item+1)*cross_val_part_size])\n",
    "    Y_train=np.vstack((ratings[0:item*cross_val_part_size],ratings[(item+1)*cross_val_part_size:ratings.shape[0]]))\n",
    "    #print(np.shape(X_train))   \n",
    "    #print(np.shape(Y_train))   \n",
    "    start = time.time()\n",
    "    omega_item,omega0_item,V_item=mini_batch_gradient_descent(X_train,Y_train,K=3,learning_rate = 0.01,num_epoches=3,\n",
    "                                                            batch_size=100000)\n",
    "    stop = time.time()\n",
    "    duration = stop-start\n",
    "    print(duration)\n",
    "    sum_const=X_test.dot(V_item)                     \n",
    "    ratings_predict=svd_predict(X_test,omega_item,omega0_item,V_item,sum_const)\n",
    "    r2.insert(item,r_squared(ratings_predict,Y_test))\n",
    "    rmse.insert(item,root_mean_square_error(ratings_predict,Y_test))\n",
    "    sum_const=X_train.dot(V_item)                     \n",
    "    ratings_predict=svd_predict(X_train,omega_item,omega0_item,V_item,sum_const)                    \n",
    "    r2_train.insert(item,r_squared(ratings_predict,Y_train))\n",
    "    rmse_train.insert(item,root_mean_square_error(ratings_predict,Y_train))\n",
    "    print(r2[item],rmse[item],r2_train[item],rmse_train[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>E</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>RMSE</td>\n",
       "      <td>1.079378</td>\n",
       "      <td>1.079389</td>\n",
       "      <td>1.079480</td>\n",
       "      <td>1.079398</td>\n",
       "      <td>1.079431</td>\n",
       "      <td>1.079415</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>R^2</td>\n",
       "      <td>0.010663</td>\n",
       "      <td>0.010659</td>\n",
       "      <td>0.010674</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.010665</td>\n",
       "      <td>0.010666</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RMSE train</td>\n",
       "      <td>1.079422</td>\n",
       "      <td>1.079422</td>\n",
       "      <td>1.079399</td>\n",
       "      <td>1.079418</td>\n",
       "      <td>1.079408</td>\n",
       "      <td>1.079413</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>R^2 train</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>0.010668</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1         2         3         4         5         E  \\\n",
       "RMSE        1.079378  1.079389  1.079480  1.079398  1.079431  1.079415   \n",
       "R^2         0.010663  0.010659  0.010674  0.010671  0.010665  0.010666   \n",
       "RMSE train  1.079422  1.079422  1.079399  1.079418  1.079408  1.079413   \n",
       "R^2 train   0.010673  0.010675  0.010664  0.010668  0.010672  0.010670   \n",
       "\n",
       "                  SD  \n",
       "RMSE        0.000037  \n",
       "R^2         0.000006  \n",
       "RMSE train  0.000008  \n",
       "R^2 train   0.000004  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise data of lists. \n",
    "data = {'1':[rmse[0],r2[0],rmse_train[0],r2_train[0]], \n",
    "        '2':[rmse[1],r2[1],rmse_train[0],r2_train[1]],\n",
    "        '3':[rmse[2],r2[2],rmse_train[2],r2_train[2]],\n",
    "        '4':[rmse[3],r2[3],rmse_train[3],r2_train[3]],\n",
    "        '5':[rmse[4],r2[4],rmse_train[4],r2_train[4]],\n",
    "        'E':[np.mean(rmse),np.mean(r2),np.mean(rmse_train),np.mean(r2_train)],\n",
    "        'SD':[np.std(rmse),np.std(r2),np.std(rmse_train),np.std(r2_train)]} \n",
    " \n",
    "# Creates pandas DataFrame. \n",
    "df_res = pd.DataFrame(data, index =['RMSE', 'R^2','RMSE train','R^2 train']) \n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
