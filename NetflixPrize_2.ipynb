{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from scipy.sparse import coo_matrix, hstack,vstack\n",
    "import time\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Unnamed: 0  User_Id  Rating  Movie_Id\n",
      "0                   1  1488844     3.0         1\n",
      "5000000       5000996   501954     2.0       996\n",
      "10000000     10001962   404654     5.0      1962\n",
      "15000000     15002876   886608     2.0      2876\n",
      "20000000     20003825  1193835     2.0      3825\n",
      "25000000     25004661  1899206     3.0      4661\n",
      "30000000     30005496   154804     4.0      5496\n",
      "35000000     35006274  2078749     5.0      6274\n",
      "40000000     40007057   450763     5.0      7057\n",
      "45000000     45007991   102092     3.0      7991\n",
      "50000000     50009023   220298     5.0      9023\n",
      "55000000     55010042   550530     5.0     10042\n",
      "60000000     60011038   222570     3.0     11038\n",
      "65000000     65011875  1273080     5.0     11875\n",
      "70000000     70012676  2026970     5.0     12676\n",
      "75000000     75013582   506044     4.0     13582\n",
      "80000000     80014453   353605     2.0     14453\n",
      "85000000     85015116   664606     3.0     15116\n",
      "90000000     90016008  2213715     3.0     16008\n",
      "95000000     95016879  1589401     5.0     16879\n",
      "100000000   100017627  2314006     4.0     17627\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"NetflixProcessedDataFile.csv\").drop_duplicates()\n",
    "print(df.iloc[::5000000, :])#START:STOP:STEP (5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of column User_id\n",
    "#one_hot_users = pd.get_dummies(df1['User_Id'])\n",
    "#one_hot_users\n",
    "# Drop column User_Id as it is now encoded\n",
    "#df1 = df1.drop('User_Id',axis = 1)\n",
    "# Join the encoded df\n",
    "#df1 = df1.join(one_hot_users)\n",
    "#df1  \n",
    "\n",
    "#This approach with dummies wouldnt work, because dataset is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100480507, 17770)\n"
     ]
    }
   ],
   "source": [
    "# importing one hot encoder from sklearn \n",
    "encoder = OneHotEncoder(categories='auto') \n",
    "#X-train matrix for encoder to learn categories\n",
    "#enc.fit(X)  #process of encoder learning categories\n",
    "one_hot_user_matrix = encoder.fit_transform(np.asarray(df['User_Id']).reshape(-1,1)) \n",
    "one_hot_movie_matrix = encoder.fit_transform(np.asarray(df['Movie_Id']).reshape(-1,1)) \n",
    "print(one_hot_movie_matrix.shape) #amount of rows=amount of ratings , amount of columns=amount of unique user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100480507, 497959)\n",
      "(100480507, 1)\n"
     ]
    }
   ],
   "source": [
    "X=hstack([one_hot_user_matrix,one_hot_movie_matrix]) #24053764x475257 matrix joined user_id and movie_id 2 groups\n",
    "#475257=amount of users+amount of films\n",
    "#24053764=amount of non null ratings\n",
    "#create ratings vector \n",
    "ratings=np.asarray(df['Rating']).reshape(-1,1)\n",
    "#ratings=df['Rating']\n",
    "print(X.shape)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def svd_predict(X_input,omega,omega0,V,sum_const):\n",
    "#     #linear_part=omega0+X.dot(omega)\n",
    "#     #V_part=(X.dot(V)**2)-X.power(2).dot(V**2) # (24053764,3)\n",
    "#     #return omega0+X.dot(omega)+(0.5*np.sum(np.square(sum_const)-X.power(2).dot(np.square(V)),axis=1))# (24053764,1)\n",
    "#     return omega0 + X_input.dot(omega) + (0.5 * np.sum((sum_const**2) - (X_input.power(2)).dot(V**2), axis=1)) \n",
    "def svd_predict(X,omega,omega0,V,sum_const):\n",
    "    #omega (475257,1)\n",
    "    #omega0 goes to (24053764,1) in sum \n",
    "    # V (475257,3)\n",
    "    # X (24053764, 475257)\n",
    "    prod=X.dot(omega)#(24053764,1)\n",
    "    linear_part=omega0+prod\n",
    "    #V_part=np.square(sum_const)-X.power(2).dot(np.square(V)) # (24053764,3)\n",
    "    V_part=(sum_const**2)-X.power(2).dot((V)**2) \n",
    "    sqr_part=(0.5*np.sum(V_part,axis=1)).reshape(-1,1) # (24053764,1)\n",
    "    return linear_part+sqr_part # (24053764,1)\n",
    "    \n",
    "def root_mean_square_error(ratings_predict,ratings_actual):\n",
    "    #ratings_predict=svd_predict(X,omega,omega0,V)\n",
    "    #summands=np.power(ratings_actual-ratings_predict,2)\n",
    "    mse=np.sum(np.power(ratings_actual-ratings_predict,2))/len(ratings_predict)\n",
    "    return np.sqrt(mse)\n",
    "   \n",
    "def r_squared(ratings_predict,ratings_actual):\n",
    "    #ratings_predict=svd_predict(X,omega,omega0,V)\n",
    "    nom=np.sum(np.power(ratings_actual-ratings_predict,2))\n",
    "    denom=np.sum(np.power(ratings_actual-np.mean(ratings_actual,axis=0),2))\n",
    "    return 1-nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a list containing mini-batches \n",
    "def create_mini_batch(data,i, batch_size): \n",
    "    mini_batch = data.tocsr()[i * batch_size:(i + 1)*batch_size, :] \n",
    "    X_mini = mini_batch[:, :-1] \n",
    "    Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
    "        \n",
    "    if data.shape[0] % batch_size != 0: \n",
    "        mini_batch = data[i * batch_size:data.shape[0]] \n",
    "        X_mini = mini_batch[:, :-1] \n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
    "       \n",
    "    return X_mini,Y_mini\n",
    "\n",
    "def shuffle_data(matrix):\n",
    "    indices = np.arange(matrix.shape[0]) #gets the number of rows \n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_matrix = matrix[list(indices)] \n",
    "\n",
    "def mini_batch_gradient_descent(X_train,Y_train,K,learning_rate,num_epoches,batch_size):\n",
    "    omega=np.random.sample((X_train.shape[1],1)) \n",
    "    #omega=np.array([0.5]*(X_train.shape[1]))\n",
    "    omega0=0.5\n",
    "    #V=np.random.sample((X_train.shape[1],K))\n",
    "    V=(np.array([0.5]*(X_train.shape[1]*K)).reshape(X_train.shape[1],K))\n",
    "    r2_tr=[]\n",
    "    rmse_tr=[]\n",
    "   \n",
    "    it=0\n",
    "    for it in range(num_epoches): \n",
    "        #data = sparse.hstack((X_train, Y_train)).tocsr()\n",
    "        #np.random.shuffle(data) #shuffle at each epoch (not every batch creation)\n",
    "        #shuffle_data(data)\n",
    "        X_train,Y_train=shuffle(X_train,Y_train)\n",
    "        n_minibatches = X_train.shape[0] // batch_size \n",
    "        i = 0\n",
    "        for i in range(n_minibatches + 1):\n",
    "            X_mini=X_train.tocsr()[i * batch_size:(i + 1)*batch_size, :] \n",
    "            y_mini=Y_train[i * batch_size:(i + 1)*batch_size,:]\n",
    "            #print(np.shape(X_mini))   \n",
    "            #print(np.shape(y_mini)) \n",
    "            N=X_mini.shape[0] # 24053764\n",
    "            #PARAMETERS\n",
    "            sum_const=X_mini.dot(V)\n",
    "            ratings_predict=svd_predict(X_mini,omega,omega0,V,sum_const)\n",
    "            error=y_mini-ratings_predict #(24053764,1)\n",
    "            omega=omega+learning_rate*2*((X_mini.T).dot(error))/N #(475257,1)\n",
    "            omega0=omega0+learning_rate*2*np.sum(error)/N #(1,1)\n",
    "            V = V + learning_rate*2*((X_mini.T.dot(np.multiply(error,sum_const)))-np.multiply(V, X_mini.T.dot(error)))/N\n",
    "#             for l in range(K):\n",
    "#                 for j in range (N):\n",
    "#                     Xj_col=X_mini.tocsr()[:,j]#(24053764, 1)\n",
    "#                     dV1=np.multiply(Xj_col,np.sum(X_mini*V[j][l],axis=1).reshape(-1,1))\n",
    "                \n",
    "#                     dV2=V[j][l]*Xj_col.power(2)#(24053764, 1)\n",
    "#                     dV=(dV1-dV2)\n",
    "                    \n",
    "#                     dV=(dV1-dV2)\n",
    "#                     a=np.asarray((error.T).dot(dV)).reshape(-1,1)\n",
    "#                     print(a)\n",
    "#                     print(V[j][l]+a)\n",
    "#                     #print(np.shape(dV))\n",
    "#                     #print((learning_rate*2*((error.T).dot(dV))/N).to_dense)\n",
    "#                     #print (V)\n",
    "#                     #print(np.shape(V[j][l].reshape(-1))\n",
    "#                     #print(np.shape(V[j][l].reshape(1,-1).sum(learning_rate*2*(error.T).dot(dV).reshape(1,-1)/N)))\n",
    "                    \n",
    "#                     V[j][l]=V[j][l]+learning_rate*2*((error.T).dot(dV))/N #(1,1)\n",
    "            ratings_predict=svd_predict(X_mini,omega,omega0,V,sum_const)\n",
    "            r2_tr.append(r_squared(ratings_predict, y_mini))\n",
    "            rmse_tr.append(root_mean_square_error(ratings_predict, y_mini))\n",
    "            #print(rmse_tr)\n",
    "            #print(r2_tr)\n",
    "    return omega,omega0,V\n",
    "\n",
    "# error=ratings-svd_predict(X,omega,omega0,V)\n",
    "# N=X.shape[0]\n",
    "# learning_rate=0.001\n",
    "# omega=omega+learning_rate*2*((X.T).dot(error))/N #(475257,1)\n",
    "# omega0=omega0+learning_rate*2*np.sum(error)/N\n",
    "# for l in range(3):\n",
    "#     for j in range (N):\n",
    "#         Xj_col=X.tocsr()[:,j] #(24053764, 1)\n",
    "#         #print(np.shape(np.multiply(Xj_col,np.sum(X*V[j][l],axis=1).reshape(-1,1)))) #(24053764, 1)\n",
    "#         #print(np.shape(V[j][l]*np.square(Xj_col)))\n",
    "#         dV=np.multiply(Xj_col,np.sum(X*V[j][l],axis=1).reshape(-1,1))-V[j][l]*np.square(Xj_col)\n",
    "#         #print(np.shape((error.T).dot(dV))) #(1, 1)\n",
    "#         V[j][l]=V[j][l]+learning_rate*2*((error.T).dot(dV))/N #(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471.74917697906494\n",
      "-0.15281695097850712 1.1646612214560201 -0.11561538536681937 1.1463648757897293\n",
      "481.4578561782837\n",
      "-0.1531907358813609 1.1661001236333077 -0.1192288546677085 1.1478479932780807\n",
      "478.0649871826172\n",
      "-0.14495764794730603 1.1677591551373108 -0.12369158337504182 1.1487229854198209\n",
      "541.4026155471802\n",
      "-0.1326883210204901 1.1528585049059301 -0.11484886233758407 1.1462018500405524\n",
      "519.696811914444\n",
      "-0.1300558592810519 1.1481567942006532 -0.1265319713873574 1.153189033653985\n"
     ]
    }
   ],
   "source": [
    "r2=[]\n",
    "rmse=[]\n",
    "omega=[]\n",
    "omega0=[]\n",
    "r2_train=[]\n",
    "rmse_train=[]\n",
    "cross_val_parts_amount=5\n",
    "cross_val_part_size=X.shape[0]//cross_val_parts_amount\n",
    "item=0\n",
    "for item in range(cross_val_parts_amount):\n",
    "    X_test=X.tocsr()[item*cross_val_part_size:(item+1)*cross_val_part_size,:]\n",
    "    Y_test=ratings[item*cross_val_part_size:(item+1)*cross_val_part_size]\n",
    "    X_train=vstack([X.tocsr()[0:(item)*cross_val_part_size,:], X.tocsr()[(item+1)*cross_val_part_size:X.shape[0],:]], 'csr') \n",
    "    #Y_train=np.delete(ratings,ratings[item*cross_val_part_size:(item+1)*cross_val_part_size])\n",
    "    #Y_train=ratings.drop(ratings.index[item*cross_val_part_size:(item+1)*cross_val_part_size])\n",
    "    Y_train=np.vstack((ratings[0:item*cross_val_part_size],ratings[(item+1)*cross_val_part_size:ratings.shape[0]]))\n",
    "    #print(np.shape(X_train))   \n",
    "    #print(np.shape(Y_train))   \n",
    "    start = time.time()\n",
    "    omega_item,omega0_item,V_item=mini_batch_gradient_descent(X_train,Y_train,K=3,learning_rate = 0.01,num_epoches=3,\n",
    "                                                            batch_size=100000)\n",
    "    stop = time.time()\n",
    "    duration = stop-start\n",
    "    print(duration)\n",
    "    sum_const=X_test.dot(V_item)                     \n",
    "    ratings_predict=svd_predict(X_test,omega_item,omega0_item,V_item,sum_const)\n",
    "    r2.insert(item,r_squared(ratings_predict,Y_test))\n",
    "    rmse.insert(item,root_mean_square_error(ratings_predict,Y_test))\n",
    "    sum_const=X_train.dot(V_item)                     \n",
    "    ratings_predict=svd_predict(X_train,omega_item,omega0_item,V_item,sum_const)                    \n",
    "    r2_train.insert(item,r_squared(ratings_predict,Y_train))\n",
    "    rmse_train.insert(item,root_mean_square_error(ratings_predict,Y_train))\n",
    "    print(r2[item],rmse[item],r2_train[item],rmse_train[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>E</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>RMSE</td>\n",
       "      <td>1.164661</td>\n",
       "      <td>1.166100</td>\n",
       "      <td>1.167759</td>\n",
       "      <td>1.152859</td>\n",
       "      <td>1.148157</td>\n",
       "      <td>1.159907</td>\n",
       "      <td>0.007879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>R^2</td>\n",
       "      <td>-0.152817</td>\n",
       "      <td>-0.153191</td>\n",
       "      <td>-0.144958</td>\n",
       "      <td>-0.132688</td>\n",
       "      <td>-0.130056</td>\n",
       "      <td>-0.142742</td>\n",
       "      <td>0.009773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RMSE train</td>\n",
       "      <td>1.146365</td>\n",
       "      <td>1.146365</td>\n",
       "      <td>1.148723</td>\n",
       "      <td>1.146202</td>\n",
       "      <td>1.153189</td>\n",
       "      <td>1.148465</td>\n",
       "      <td>0.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>R^2 train</td>\n",
       "      <td>-0.115615</td>\n",
       "      <td>-0.119229</td>\n",
       "      <td>-0.123692</td>\n",
       "      <td>-0.114849</td>\n",
       "      <td>-0.126532</td>\n",
       "      <td>-0.119983</td>\n",
       "      <td>0.004531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1         2         3         4         5         E  \\\n",
       "RMSE        1.164661  1.166100  1.167759  1.152859  1.148157  1.159907   \n",
       "R^2        -0.152817 -0.153191 -0.144958 -0.132688 -0.130056 -0.142742   \n",
       "RMSE train  1.146365  1.146365  1.148723  1.146202  1.153189  1.148465   \n",
       "R^2 train  -0.115615 -0.119229 -0.123692 -0.114849 -0.126532 -0.119983   \n",
       "\n",
       "                  SD  \n",
       "RMSE        0.007879  \n",
       "R^2         0.009773  \n",
       "RMSE train  0.002542  \n",
       "R^2 train   0.004531  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise data of lists. \n",
    "data = {'1':[rmse[0],r2[0],rmse_train[0],r2_train[0]], \n",
    "        '2':[rmse[1],r2[1],rmse_train[0],r2_train[1]],\n",
    "        '3':[rmse[2],r2[2],rmse_train[2],r2_train[2]],\n",
    "        '4':[rmse[3],r2[3],rmse_train[3],r2_train[3]],\n",
    "        '5':[rmse[4],r2[4],rmse_train[4],r2_train[4]],\n",
    "        'E':[np.mean(rmse),np.mean(r2),np.mean(rmse_train),np.mean(r2_train)],\n",
    "        'SD':[np.std(rmse),np.std(r2),np.std(rmse_train),np.std(r2_train)]} \n",
    " \n",
    "# Creates pandas DataFrame. \n",
    "df_res = pd.DataFrame(data, index =['RMSE', 'R^2','RMSE train','R^2 train']) \n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20096101, 497959)\n",
      "(20096101, 497959)\n",
      "(20096101, 497959)\n",
      "(20096101, 497959)\n",
      "(20096101, 497959)\n"
     ]
    }
   ],
   "source": [
    "cross_val_part_size=X.shape[0]//(cross_val_parts_amount)\n",
    "cross_val_part_size\n",
    "j=0\n",
    "for j in range(cross_val_parts_amount):\n",
    "    X_test=X.tocsr()[item*cross_val_part_size:(item+1)*cross_val_part_size,:]\n",
    "    print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
